#서울시 응답소(원순씨를 부탁) 크롤링
import urllib.request
from bs4 import BeautifulSoup
import re
import os

list_url = "http://eungdapso.seoul.go.kr/Shr/Shr01/Shr01_lis.jsp"
detail_url = "http://eungdapso.seoul.go.kr/Shr/Shr01/Shr01_vie.jsp"

maxPage=0  #최대 페이지 지정하기 위한 변수

def get_save_path():  #파일 저장하는 위치를 지정하는 함수
    save_path = str(input("저장할 위치와 파일명을 적어주세요 : ")) #파일 저장위치 설정
    #ex) C:\Python34\crolling\텍스트명.txt
    save_path = save_path.replace("\\","/") #저장한 설정(\)을 (/)로 교체

    if not os.path.isdir(os.path.split(save_path)[0]): #사용자가 입력한 경로에 폴더가 존재하는지 판단 여부
        os.mkdir(os.path.split(save_path)[0]) #없다면 해당경로에 폴더 생성

    return save_path  #경로 설정 완료~

def fetch_list_url(maxPage): #응답소의 민원 리스트 페이지에서 민원들을 가져오는 역할을 하는 함수

    params = []   #파라미터(즉 리스트의 주소)를 저장할 리스트를 생성
    
    for x in range(1,maxPage+1):  #1페이지 부터 최대페이지까지를 for문으로 반복

        request_header = urllib.parse.urlencode({"page" : str(x)}) #urlencode함수를 이용해서 딕셔너리의 형태의 데이터를 인코딩된 문자열의 형태로 변환해서 request_header라는 변수에 넣겠다.

        request_header = request_header.encode("utf-8") #request_header라는 변수에 들어있는 값(url page겠죠?)을 utf-8(한글)형식으로 변환

        url = urllib.request.Request(list_url, request_header) #응답소의 민원 리스트 페이지에 url page를 전달

        res = urllib.request.urlopen(url).read().decode("utf-8") #리스트 페이지를 불러와서 utf-8형식으로 변환

        #여기까지의 내용은 리스트에 있는 제목들이 링크로 연결되어 있는 것을 알 수 있는데(제목 누르면 그 창으로 들어가지는 것) 한 페이지당 그 링크들의 주소를 변환하고 전달하고 하는 과정이다.

        bs = BeautifulSoup(res, "html.parser") #리스트 페이지를 BeautifulSoup 객체에 넣어서 필터링을 할 준비 개시

        listbox = bs.find_all("li", class_="pclist_list_tit2") #응답소 리스트에는 민원의 제목에 파리미터로 쓰일 값이 존재하기 때문에 그 태그가 <li>태그안에 있어서 <li>태그를 모두 가져온다.

        for i in listbox: #for문을 이용해서 <li>태그 안에 있는 파라미터를 정규식으로 뽑아내서 리스트에 추가!
            params.append(re.search("[0-9]{14}", i.find("a")["href"]).group()) #리스트 들의 제목들(링크가 걸려있는)을 params에 추가로 넣어줌 
            print(params) #params가 저장되는 과정을 볼 수 있음

    return params #리스트 완료~

def fetch_detail_url(inputPage): #민원의 내용이 들어있는 페이지를 열어서 민원의 내용을 가져오기 위한 함수
    params2 = []  #리스트 변수 하나를 생성
    
    params2 = fetch_list_url(inputPage) #fetch_list_url함수를 실행해서 주소값을 얻어 params2라는 리스트변수에 저장
        
    f = open(get_save_path(), 'w', encoding="utf-8") #get_save_path함수를 실행(파일저장하는 곳 설정) -> 파일 쓰기모드 실행 -> encoding인수의 역할은 파일의 내용을 쓸 때 utf-8형식으로 내용을 쓰겠다.(메모장에 utf-8로 저장해본거 기억하시죠?)

    for p in params2: #params 리스트변수를 이용해서 for문을 실행

        request_header = urllib.parse.urlencode({"RCEPT_NO" : str(p)}) #민원 페이지를 얻기 위한 값을 인코딩 시켜 저장
        request_header = request_header.encode("utf-8") #header를 utf-8로 변환하여 저장

        url = urllib.request.Request(detail_url, request_header) #민원 내용이 있는 페이지에 파라미터를 전달
        res = urllib.request.urlopen(url).read().decode("utf-8") #민원 내용 페이지를 열러서 데이터를 읽어온다(utf-8 -> 한글)

        bs = BeautifulSoup(res, "html.parser") #읽어 들인 데이터를 BeautifulSoup 객체로 생성하여 필터링할 준비를 함.
        div = bs.find("div", class_="form_table") #민원 내용이 적혀있는 <div>태그의 내용을 가져온다.

        tables = div.find_all("table") #<div>태그 안에 <table>태그로 내용이 분류되어 있어서 <div>태그 안에 모든 <table>태그를 가져온다.

        info = tables[0].find_all("td") #첫번째 <table>태그 안에서(tables[0] = 첫번째, [1]이면 2번째겠죠?) <td>태그안에 있는 내용을 가져온다.

        title = info[0].get_text(strip=True) #첫번째 <td>태그는 민원 제목이 있음 -> <td>태그의 텍스트를 공백을 제거해서 가져옴
        date = info[1].get_text(strip=True)  #두번째 <table>태그에는 날짜가 있음 -> <td>태그의 텍스트를 공백을 제거해서 가져옴

        question= tables[1].find("div", class_="table_inner_desc").get_text(strip=True) #두번째 <table>태그에는 민원의 내용이 있음 -> <table>태그에서 <div>태그를 찾아와 공백을 제거하고 텍스트를 가져옴
        answer = tables[2].find("div", class_="table_inner_desc").get_text(strip=True)  #세번째 <table>태그에는 민원에 대한 답변이 있음 -> <table>태그에서 <div>태그를 찾아와 공백을 제거하고 텍스트를 가져옴
        
        f.write("==" * 30 + "\n")  #변수에 저장된 텍스트들을 차례대로 구분해서 파일에 쓴다. -> 이거는 구분을 하기 위한 표시

        f.write(title + "\n")
        f.write(date + "\n")
        f.write(question +"\n")
        f.write(answer + "\n")

        f.write("==" * 30 + "\n")

num = int(input("몇 페이지까지 크롤링? "))  #main메소드이고 원하는 숫자를 입력하면 됨
fetch_detail_url(num)  #함수에 사용자가 원하는 숫자를 대입
